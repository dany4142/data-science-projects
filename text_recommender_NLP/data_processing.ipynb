{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data preprocessing to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from scipy.sparse import vstack, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_raw_df(df):\n",
    "    \"\"\"\n",
    "    Cleanup data and join questions to answers\n",
    "    :param df: raw DataFrame\n",
    "    :return: processed DataFrame\n",
    "    \"\"\"\n",
    "    df['PostTypeId'] = df['PostTypeId'].astype(int)\n",
    "    df['Id'] = df['Id'].astype(int)\n",
    "    df['AnswerCount'] = df['AnswerCount'].fillna(-1)\n",
    "    df['AnswerCount'] = df['AnswerCount'].astype(int)\n",
    "    df['OwnerUserId'].fillna(-1, inplace=True)\n",
    "    df['OwnerUserId'] = df['OwnerUserId'].astype(int)\n",
    "    df.set_index('Id', inplace = True, drop = False)\n",
    "    df['is_question'] = df['PostTypeId'] == 1\n",
    "    \n",
    "    df = df[df['PostTypeId'].isin([1,2])]\n",
    "    df = df.join(\n",
    "        df[['Id','Title','body_text','Score', 'AcceptedAnswerId']],\n",
    "        on = 'ParentId', how = 'left', rsuffix = '_question',\n",
    "    \n",
    "    )\n",
    "    return df\n",
    "\n",
    "def add_v1_features(df):\n",
    "    \"\"\"\n",
    "    Add our first features to an input DataFrame\n",
    "    :param df: DataFrame of questions\n",
    "    :return: DataFrame with added feature columns\n",
    "    \"\"\"\n",
    "    df['action_verb_full'] = (\n",
    "        df['full_text'].str.contains('can',regex = False)\n",
    "        | df['full_text'].str.contains('What', regex = False)\n",
    "        | df['full_text'].str.contains('should', regex = False)\n",
    "    \n",
    "    )\n",
    "    \n",
    "    df['language_question'] = (\n",
    "        df['full_text'].str.contains('punctuate', regex = False)\n",
    "        | df['full_text'].str.contains('capitalize', regex = False)\n",
    "        | df['full_text'].str.contains('should', regex = False)\n",
    "    \n",
    "    )\n",
    "    df['question_mark_full'] = df['full_text'].str.contains('?', regex = False)\n",
    "    df['text_len'] = df['full_text'].str.len()\n",
    "    return df\n",
    "\n",
    "def train_vectorizer(df):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        strip_accents = 'ascii', min_df = 5, max_df = 0.5, max_features = 10000\n",
    "    )\n",
    "    vectorizer.fit(df['full_text'].copy())\n",
    "    return vectorizer\n",
    "\n",
    "def get_vectorized_series(text_series, vectorizer):\n",
    "    vectors = vectorizer.transform(text_series)\n",
    "    vectorized_series = [vectors[i] for i in range(vectors.shape[0])]\n",
    "    return vectorized_series\n",
    "\n",
    "def add_text_features_to_df(df):\n",
    "    df['full_text'] = df['Title'].str.cat(df['body_text'], sep=' ', na_rep = '')\n",
    "    df = add_v1_features(df.copy())\n",
    "    return df\n",
    "\n",
    "def get_vectorized_inputs_and_labels(df):\n",
    "    vectorized_features = np.append(\n",
    "        np.vstack(df['vectors']),\n",
    "        df[\n",
    "            [\n",
    "                'action_verb_full', 'question_mark_full',\n",
    "                'norm_text_length', 'language_question'\n",
    "            ]\n",
    "        ],1,\n",
    "    \n",
    "    )\n",
    "    label = df['Score']> df['Score'].median()\n",
    "    return vectorized_features, label\n",
    "\n",
    "def get_feature_vector_and_label(df, feature_names):\n",
    "    vec_features = vstack(df['vectors'])\n",
    "    num_features = df[feature_names].astype(float)\n",
    "    features = hstack([vec_features, num_features])\n",
    "    labels = df['Score']>df['Score'].median()\n",
    "    return features, labels\n",
    "\n",
    "def get_normalized_series(df, col):\n",
    "    return (df[col]-df[col].mean())/df[col].std()\n",
    "\n",
    "def get_random_train_test_split(posts, test_size = 0.3, random_state = 40):\n",
    "    return train_test_split(\n",
    "        posts, test_size = test_size, random_state = random_state\n",
    "    \n",
    "    )\n",
    "\n",
    "def get_split_by_author(\n",
    "    posts, author_id_columns = 'OwnerUserId', test_size = 0.3, random_state = 40\n",
    "):\n",
    "    splitter = GroupShuffleSplit(\n",
    "        n_splits = 1, test_size = test_size, random_state = random_state\n",
    "    )\n",
    "    splits = splitter.split(posts, groups=posts[author_id_columns])\n",
    "    train_idx, test_idx = next(splits)\n",
    "    return posts.iloc[train_idx, :], posts.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
