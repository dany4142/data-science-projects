{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZOMATO PARSING PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Important Highlights about the Project\n",
    "\n",
    "The primary features available on Zomato's platform can be broadly divided into four different categories.\n",
    "1. Summary Highlights of Restaurant and Cuisine w.r.t demographics\n",
    "2. Restaurant Attributes, i.e. Rating, Review Counts, Type of Cuisines Offered and Discounts. \n",
    "3. Menu details, price points and dish-types.\n",
    "4. Reviews and sentiments of individuals about food. \n",
    "\n",
    "For this phase of the project, I will only parse and analyze data for the first two categories. The second set of features will be parsed and analyzed in the second phase to study the distribution of menu items, major restaurant offerings and price points. \n",
    "\n",
    "The fourth feature of reviews is also precious as it provides a gateway to analyze the sentiments of individuals and their remarks about various foods, cuisines and restaurants. Combined with the cuisine offering and segmentation, an NLP model can reveal important insights to steer brand and marketing activities to gain more traction and higher sales recommendations. \n",
    "\n",
    "#### PROJECT DESIGN\n",
    "\n",
    "The first part of the project starts with parsing the key features, structuring it and inserting it in a schema of 4 tables, which are broadly designed based on the type of data points. \n",
    "\n",
    "As this overall analysis is granular to the demographics level, the table has been assigned id column as many to many relationship signifying the area they belong to. The reason I have used database to store and retrieve data in the intermediate step is to reduce any data loss due to server errors or any complications that might arise as parser runs. A database also provides more structured tabular form to store data and can help do some hands-on ETL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import re\n",
    "import mysql.connector\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError\n",
    "import datetime\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import codecs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting and defining cursors for connection with database \n",
    "headers = {'User-Agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36\"}\n",
    "mydb = mysql.connector.connect(host = 'localhost', user='root', password='*****')\n",
    "cur = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('use zomato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('drop table if exists cuisine_summary')\n",
    "cur.execute('drop table if exists time_summary')\n",
    "cur.execute('drop table if exists minimum_requirement')\n",
    "cur.execute('''drop table if exists restaurants''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining schema of the project database \n",
    "cur.execute('''Create table if not exists cuisine_summary(\n",
    "id int not null, \n",
    "area_name varchar(128),\n",
    "cuisine_type varchar(128),\n",
    "cuisine_count varchar(128)\n",
    ")''')\n",
    "\n",
    "cur.execute('''create table if not exists time_summary(\n",
    "id int not null,\n",
    "area_name varchar(128),\n",
    "expected_delivery_time varchar(128),\n",
    "count_of_restaurants varchar(128)\n",
    ")''')\n",
    "\n",
    "cur.execute('''create table if not exists minimum_requirement(\n",
    "id int not null, \n",
    "area_name varchar(128),\n",
    "minimum_order_type varchar(128),\n",
    "minimum_order_count varchar(128)\n",
    ")''')\n",
    "\n",
    "cur.execute('''Create table if not exists restaurants(\n",
    "id int not null,\n",
    "area_name varchar(128),\n",
    "restaurant_name varchar(128),\n",
    "restaurant_rating varchar(128),\n",
    "review_count varchar(128),\n",
    "offered_cuisines varchar(128),\n",
    "average_cost_per_person varchar(128),\n",
    "expected_delivery_time varchar(128),\n",
    "minimum_order_requirement varchar(128)\n",
    ")''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zomato did not have any clear distinction or pattern for the zone numbering, so I had to manually fetch the area codes on their website\n",
    "#While chosing the codes I made sure I am making a fair and equal distribution of all the areas in dubai\n",
    "\n",
    "delivery_zones = [3831, 3832, 3835, 42656, 7980, 3836, 3850, 3790, 4716, 3849, 3882, 3847, 3878, 3879, 8458,42643, 3880, 3786, 3787, 3788, 8465, 8466, \n",
    "                 7721, 7722, 7723, 3839, 3840, 3844, 3801, 3802, 4721, 4720, 4723, 3797, 4724, 3794, 7990, 3816, 3821, 3822, 7986, 3820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Area 1, Page Number 1\n",
      "Parsing Area 1, Page Number 2\n",
      "Parsing Area 1, Page Number 3\n",
      "Parsing Area 1, Page Number 4\n",
      "Parsing Area 2, Page Number 1\n",
      "Parsing Area 2, Page Number 2\n",
      "Parsing Area 2, Page Number 3\n",
      "Parsing Area 2, Page Number 4\n",
      "Parsing Area 3, Page Number 1\n",
      "Now I will rest for 6 Seconds and will then start working\n",
      "Parsing Area 3, Page Number 2\n",
      "Parsing Area 3, Page Number 3\n",
      "Parsing Area 3, Page Number 4\n",
      "Parsing Area 4, Page Number 1\n",
      "Parsing Area 4, Page Number 2\n",
      "Parsing Area 4, Page Number 3\n",
      "Parsing Area 4, Page Number 4\n",
      "Parsing Area 5, Page Number 1\n",
      "Parsing Area 5, Page Number 2\n",
      "Parsing Area 5, Page Number 3\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 5, Page Number 4\n",
      "Parsing Area 6, Page Number 1\n",
      "Parsing Area 6, Page Number 2\n",
      "Parsing Area 6, Page Number 3\n",
      "Parsing Area 6, Page Number 4\n",
      "Parsing Area 7, Page Number 1\n",
      "Parsing Area 7, Page Number 2\n",
      "Parsing Area 7, Page Number 3\n",
      "Parsing Area 7, Page Number 4\n",
      "Parsing Area 8, Page Number 1\n",
      "Now I will rest for 5 Seconds and will then start working\n",
      "Parsing Area 8, Page Number 2\n",
      "Parsing Area 8, Page Number 3\n",
      "Parsing Area 8, Page Number 4\n",
      "Parsing Area 9, Page Number 1\n",
      "Parsing Area 9, Page Number 2\n",
      "Parsing Area 9, Page Number 3\n",
      "Parsing Area 9, Page Number 4\n",
      "Parsing Area 10, Page Number 1\n",
      "Parsing Area 10, Page Number 2\n",
      "Parsing Area 10, Page Number 3\n",
      "Now I will rest for 4 Seconds and will then start working\n",
      "Parsing Area 10, Page Number 4\n",
      "Parsing Area 11, Page Number 1\n",
      "Parsing Area 11, Page Number 2\n",
      "Parsing Area 11, Page Number 3\n",
      "Parsing Area 11, Page Number 4\n",
      "Parsing Area 12, Page Number 1\n",
      "Parsing Area 12, Page Number 2\n",
      "Parsing Area 12, Page Number 3\n",
      "Parsing Area 12, Page Number 4\n",
      "Parsing Area 13, Page Number 1\n",
      "Now I will rest for 4 Seconds and will then start working\n",
      "Parsing Area 13, Page Number 2\n",
      "Parsing Area 13, Page Number 3\n",
      "Parsing Area 13, Page Number 4\n",
      "Parsing Area 14, Page Number 1\n",
      "Parsing Area 14, Page Number 2\n",
      "Parsing Area 14, Page Number 3\n",
      "Parsing Area 14, Page Number 4\n",
      "Parsing Area 15, Page Number 1\n",
      "Parsing Area 15, Page Number 2\n",
      "Parsing Area 15, Page Number 3\n",
      "Now I will rest for 5 Seconds and will then start working\n",
      "Parsing Area 15, Page Number 4\n",
      "Parsing Area 16, Page Number 1\n",
      "Parsing Area 16, Page Number 2\n",
      "Parsing Area 16, Page Number 3\n",
      "Parsing Area 16, Page Number 4\n",
      "Parsing Area 17, Page Number 1\n",
      "Parsing Area 17, Page Number 2\n",
      "Parsing Area 17, Page Number 3\n",
      "Parsing Area 17, Page Number 4\n",
      "Parsing Area 18, Page Number 1\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 18, Page Number 2\n",
      "Parsing Area 18, Page Number 3\n",
      "Parsing Area 18, Page Number 4\n",
      "Parsing Area 19, Page Number 1\n",
      "Parsing Area 19, Page Number 2\n",
      "Parsing Area 19, Page Number 3\n",
      "Parsing Area 19, Page Number 4\n",
      "Parsing Area 20, Page Number 1\n",
      "Parsing Area 20, Page Number 2\n",
      "Parsing Area 20, Page Number 3\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 20, Page Number 4\n",
      "Parsing Area 21, Page Number 1\n",
      "Parsing Area 21, Page Number 2\n",
      "Parsing Area 21, Page Number 3\n",
      "Parsing Area 21, Page Number 4\n",
      "Parsing Area 22, Page Number 1\n",
      "Parsing Area 22, Page Number 2\n",
      "Parsing Area 22, Page Number 3\n",
      "Parsing Area 22, Page Number 4\n",
      "Parsing Area 23, Page Number 1\n",
      "Now I will rest for 5 Seconds and will then start working\n",
      "Parsing Area 23, Page Number 2\n",
      "Parsing Area 23, Page Number 3\n",
      "Parsing Area 23, Page Number 4\n",
      "Parsing Area 24, Page Number 1\n",
      "Parsing Area 24, Page Number 2\n",
      "Parsing Area 24, Page Number 3\n",
      "Parsing Area 24, Page Number 4\n",
      "Parsing Area 25, Page Number 1\n",
      "Parsing Area 25, Page Number 2\n",
      "Parsing Area 25, Page Number 3\n",
      "Now I will rest for 5 Seconds and will then start working\n",
      "Parsing Area 25, Page Number 4\n",
      "Parsing Area 26, Page Number 1\n",
      "Parsing Area 26, Page Number 2\n",
      "Parsing Area 26, Page Number 3\n",
      "Parsing Area 26, Page Number 4\n",
      "Parsing Area 27, Page Number 1\n",
      "Parsing Area 27, Page Number 2\n",
      "Parsing Area 27, Page Number 3\n",
      "Parsing Area 27, Page Number 4\n",
      "Parsing Area 28, Page Number 1\n",
      "Now I will rest for 4 Seconds and will then start working\n",
      "Parsing Area 28, Page Number 2\n",
      "Parsing Area 28, Page Number 3\n",
      "Parsing Area 28, Page Number 4\n",
      "Parsing Area 29, Page Number 1\n",
      "Parsing Area 29, Page Number 2\n",
      "Parsing Area 29, Page Number 3\n",
      "Parsing Area 29, Page Number 4\n",
      "Parsing Area 30, Page Number 1\n",
      "Parsing Area 30, Page Number 2\n",
      "Parsing Area 30, Page Number 3\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 30, Page Number 4\n",
      "Parsing Area 31, Page Number 1\n",
      "Parsing Area 31, Page Number 2\n",
      "Parsing Area 31, Page Number 3\n",
      "Parsing Area 31, Page Number 4\n",
      "Parsing Area 32, Page Number 1\n",
      "Parsing Area 32, Page Number 2\n",
      "Parsing Area 32, Page Number 3\n",
      "Parsing Area 32, Page Number 4\n",
      "Parsing Area 33, Page Number 1\n",
      "Now I will rest for 6 Seconds and will then start working\n",
      "Parsing Area 33, Page Number 2\n",
      "Parsing Area 33, Page Number 3\n",
      "Parsing Area 33, Page Number 4\n",
      "Parsing Area 34, Page Number 1\n",
      "Parsing Area 34, Page Number 2\n",
      "Parsing Area 34, Page Number 3\n",
      "Parsing Area 34, Page Number 4\n",
      "Parsing Area 35, Page Number 1\n",
      "Parsing Area 35, Page Number 2\n",
      "Parsing Area 35, Page Number 3\n",
      "Now I will rest for 4 Seconds and will then start working\n",
      "Parsing Area 35, Page Number 4\n",
      "Parsing Area 36, Page Number 1\n",
      "Parsing Area 36, Page Number 2\n",
      "Parsing Area 36, Page Number 3\n",
      "Parsing Area 36, Page Number 4\n",
      "Parsing Area 37, Page Number 1\n",
      "Parsing Area 37, Page Number 2\n",
      "Parsing Area 37, Page Number 3\n",
      "Parsing Area 37, Page Number 4\n",
      "Parsing Area 38, Page Number 1\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 38, Page Number 2\n",
      "Parsing Area 38, Page Number 3\n",
      "Parsing Area 38, Page Number 4\n",
      "Parsing Area 39, Page Number 1\n",
      "Parsing Area 39, Page Number 2\n",
      "Parsing Area 39, Page Number 3\n",
      "Parsing Area 39, Page Number 4\n",
      "Parsing Area 40, Page Number 1\n",
      "Parsing Area 40, Page Number 2\n",
      "Parsing Area 40, Page Number 3\n",
      "Now I will rest for 7 Seconds and will then start working\n",
      "Parsing Area 40, Page Number 4\n",
      "Parsing Area 41, Page Number 1\n",
      "Parsing Area 41, Page Number 2\n",
      "Parsing Area 41, Page Number 3\n",
      "Parsing Area 41, Page Number 4\n",
      "Parsing Area 42, Page Number 1\n",
      "Parsing Area 42, Page Number 2\n",
      "Parsing Area 42, Page Number 3\n",
      "Parsing Area 42, Page Number 4\n"
     ]
    }
   ],
   "source": [
    "counter = 1 #to count iterations\n",
    "for id1,areas in enumerate(delivery_zones,1):\n",
    "    for page in range(1,5): # for the purpose of this project I only parsed 4 pages from a single zone\n",
    "        try:\n",
    "            new_list = []\n",
    "            url = 'https://www.zomato.com/dubai/order-food-online?delivery_subzone={}&page={}'.format(areas,page)\n",
    "            http = requests.get(url, headers = headers)\n",
    "            bs = BeautifulSoup(http.text, 'html.parser')\n",
    "            area = bs.h1.text\n",
    "            #classes for the related data points\n",
    "            cuisine = bs.find_all('div',{'class':'link_hover w100 search_filter cuisine'})\n",
    "            deliver_time = bs.find_all('div',{'class':'link_hover w100 search_filter cft cursor-pointer'})\n",
    "            res_name = bs.find_all(attrs={'data-result-type':'ResCard_Name'})\n",
    "            rating = bs.find_all(attrs={'class':'rating-value'})\n",
    "            rating_class = bs.find_all(attrs={'class':'review-count medium'})\n",
    "            desc = bs.find_all(attrs = {'class':'description'})\n",
    "            print('Parsing Area {}, Page Number {}'.format(id1, page)) #Printing to see how many iterations have been run and to debug any errors\n",
    "            counter +=1\n",
    "        except requests.exceptions.RequestException:\n",
    "            print('Encountered an Error')\n",
    "            continue\n",
    "        for x in deliver_time:\n",
    "            new_list.append(x)\n",
    "        try: #Fetching cusines summary level view and storing in its respective table\n",
    "            for x in cuisine:\n",
    "                split1 = x.text.split(' ')\n",
    "                count = ''.join(split1[-1])\n",
    "                food_type = ' '.join(split1[:-1])\n",
    "                cur.execute('''insert into cuisine_summary (id, area_name, cuisine_type, cuisine_count) values (%s, %s, %s, %s)''',\n",
    "                            (id1,area, food_type,count ))\n",
    "                mydb.commit()\n",
    "        except:\n",
    "            print('Pathway 1 Error Encountered')\n",
    "            continue\n",
    "\n",
    "        try: #fetching delivery time summary level view and storing in database\n",
    "            for x in new_list[0:3]:\n",
    "                x = x.text.split()\n",
    "                delivery_time = ' '.join(x[:4])\n",
    "                count_delivery = ''.join(x[4:])\n",
    "                cur.execute('''insert into time_summary (id, area_name, expected_delivery_time, count_of_restaurants) values (%s, %s, %s, %s)''',\n",
    "                            (id1,area, delivery_time,count_delivery ))\n",
    "                mydb.commit()  \n",
    "        except:\n",
    "            print('Pathway 2 Error Encountered')\n",
    "            continue\n",
    "        try:    #fetching minimum order data points summaries for table 3 and stroing in database\n",
    "            for x in new_list[9:]:\n",
    "                x = x.text.split()\n",
    "                min_order = ' '.join(x[:-1])\n",
    "                min_order_count = ''.join(x[-1])\n",
    "                cur.execute('''insert into minimum_requirement (id, area_name, minimum_order_type, minimum_order_count) values (%s, %s, %s, %s)''',\n",
    "                                (id1,area, min_order,min_order_count))\n",
    "                mydb.commit() \n",
    "        except:\n",
    "            print('Pathway 3 error encountered')\n",
    "            continue\n",
    "\n",
    "        try: #main table for our analysis parsing features of the restaurants and stroing in its respective table\n",
    "            for name,x, y, z in zip(res_name, rating, rating_class, desc):\n",
    "                restaurant_name = name.text.strip()\n",
    "                rate = x.text\n",
    "                reviews = y.text\n",
    "                cuisine = z.text.strip().split('\\n')\n",
    "                type_of_food = cuisine[0]\n",
    "                cost_one = cuisine[1]\n",
    "                order_values = cuisine[2]\n",
    "                minimum_order = order_values[-8:-1]\n",
    "                minimum_order1 = order_values[:-17].strip()\n",
    "                cur.execute('''insert into restaurants (id, area_name,restaurant_name, restaurant_rating, \n",
    "                review_count, offered_cuisines, average_cost_per_person,expected_delivery_time,\n",
    "                minimum_order_requirement) values (%s, %s, %s, %s, %s,%s,%s,%s,%s)''',\n",
    "                                        (id1,area, restaurant_name, rate, reviews, type_of_food, cost_one, minimum_order, minimum_order1))\n",
    "            mydb.commit()\n",
    "\n",
    "        except:\n",
    "            print('Pathway 4 error encountered')\n",
    "            continue\n",
    "        \n",
    "        if counter%10 ==0: #making the parser sleep randomly every 10 iterations so server does not find it suspicious\n",
    "            sleeper = np.random.randint(4,9)\n",
    "            print('Now I will rest for {} Seconds and will then start working'.format(sleeper))\n",
    "            time.sleep(sleeper)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE END OF THE PART FIRST\n",
    "\n",
    "This is the end of the first part and the next notebook fetches data from the sql database, puts it in a dataframe, preprocess it, and prepares the data for the final representations. Lets move to the other notebook of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
